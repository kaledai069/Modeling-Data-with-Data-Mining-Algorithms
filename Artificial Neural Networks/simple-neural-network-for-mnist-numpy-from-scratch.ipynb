{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This note contains an **explicit** routine of a neural network for MNIST digit recognizer implemented in numpy from scratch.\n",
    "\n",
    "This is a simple demonstration mainly for pedagogical purposes, which shows the basic workflow of a machine learning algorithm using a simple feedforward neural network. The derivative at the backpropagation stage is computed explicitly through the chain rule.\n",
    "\n",
    "* The model is a 3-layer feedforward neural network (FNN), in which the input layer has 784 units, and the 256-unit hidden layer is activated by ReLU, while the output layer is activated by softmax function to produce a discrete probability distribution for each input. \n",
    "\n",
    "* The loss function, model hypothesis function, and the gradient of the loss function are all implemented from ground-up in numpy in a highly vectorized fashion (no FOR loops).\n",
    "\n",
    "* The training is through a standard gradient descent with step size adaptively changing by Root Mean Square prop (RMSprop), and there is no cross-validation set reserved nor model averaging for simplicity.\n",
    "\n",
    "\n",
    "The code is vectorized and is adapted from the Softmax regression and Neural Network lectures used in [UCI Math 10](https://github.com/scaomath/UCI-Math10). \n",
    "\n",
    "Caveat lector: fluency in linear algebra and multivariate calculus.\n",
    "\n",
    "\n",
    "#### References:\n",
    "* [Stanford Deep Learning tutorial in MATLAB](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/).\n",
    "* [Learning PyTorch with examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html).\n",
    "* [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'train.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = os.listdir(\"../input\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Set up the data\n",
    "y_train = train_data['label'].values\n",
    "X_train = train_data.drop(columns=['label']).values/255\n",
    "X_test = test_data.values/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples of the input data\n",
    "We randomly choose 10 samples from the training set and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2,5, figsize=(12,5))\n",
    "axes = axes.flatten()\n",
    "idx = np.random.randint(0,42000,size=10)\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train[idx[i],:].reshape(28,28), cmap='gray')\n",
    "    axes[i].axis('off') # hide the axes ticks\n",
    "    axes[i].set_title(str(int(y_train[idx[i]])), color= 'black', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network structures\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scaomath/UCI-Math10/master/Lectures/neural_net_3l.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "The figure above is a simplication of the neural network used in this example. The circles labeled \"+1\" are the bias units. Layer 1 is the input layer, and Layer 3 is the output layer. The middle layer, Layer 2, is the hidden layer.\n",
    "\n",
    "The neural network in the figure above has 2 input units (not counting the bias unit), 3 hidden units, and 1 output unit. In this actual computation below, the input layer has 784 units, the hidden layer has 256 units, and the output layers has 10 units ($K =10$ classes).\n",
    "\n",
    "The weight matrix $W^{(0)}$ mapping input $\\mathbf{x}$ from the input layer (Layer 1) to the hidden layer (Layer 2) is of shape `(784,256)` together with a `(256,)` bias. Then $\\mathbf{a}$ is the activation from the hidden layer (Layer 2) can be written as:\n",
    "$$\n",
    "\\mathbf{a} = \\mathrm{ReLU}\\big((W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b}\\big),\n",
    "$$\n",
    "where the ReLU activation function is $\\mathrm{ReLU}(z) = \\max(z,0)$ and can be implemented in a vectorized fashion as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation function\n",
    "# THE fastest vectorized implementation for ReLU\n",
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax activation, prediction, and the loss function\n",
    "\n",
    "From the hidden layer (Layer 2) to the output layer (layer 3), the weight matrix $W^{(1)}$ is of shape `(256,10)`, the form of which is as follows:\n",
    "$$\n",
    "W^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "| & | & | & | \\\\\n",
    "\\boldsymbol{\\theta}_1 & \\boldsymbol{\\theta}_2 & \\cdots & \\boldsymbol{\\theta}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which maps the activation from Layer 2 to Layer 3 (output layer), and there is no bias because a constant can be freely added to the activation without changing the final output. \n",
    "\n",
    "At the last layer, a softmax activation is used, which can be written as follows combining the weights matrix $W^{(1)}$ that maps the activation $\\mathbf{a}$ from the hidden layer to output layer:\n",
    "$$\n",
    "P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big) = \\sigma_k(\\mathbf{a}; W^{(1)}) := \\frac{\\exp\\big(\\boldsymbol{\\theta}^{\\top}_k \\mathbf{a} \\big)}\n",
    "{\\sum_{j=1}^K \\exp\\big(\\boldsymbol{\\theta}^{\\top}_j \\mathbf{a} \\big)}.\n",
    "$$\n",
    "$\\{P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)\\}_{k=1}^K$ is the probability distribution of our model, which estimates the probability of the input $\\mathbf{x}$'s label $y$ is of class $k$. We denote this distribution by a vector \n",
    "$$\\boldsymbol{\\sigma}:= (\\sigma_1,\\dots, \\sigma_K)^{\\top}.$$\n",
    "We hope that this estimate is as close as possible to the true probability: $1_{\\{y=k\\}}$, that is $1$ if the sample $\\mathbf{x}$ is in the $k$-th class and 0 otherwise. \n",
    "\n",
    "Lastly, our prediction $\\hat{y}$ for sample $\\mathbf{x}$ can be made by choosing the class with the highest probability:\n",
    "$$\n",
    "\\hat{y} = \\operatorname{argmax}_{k=1,\\dots,K}  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big). \\tag{$\\ast$}\n",
    "$$\n",
    "\n",
    "Denote the label of the $i$-th input as $y^{(i)}$, and then the sample-wise loss function is the cross entropy measuring the difference of the distribution of this model function above with the true one $1_{\\{y^{(i)}=k\\}}$: denote $W = (W^{(0)}, W^{(1)})$, $b = (\\mathbf{b})$, let $\\mathbf{a}^{(i)}$ be the activation for the $i$-th sample in the hidden layer (Layer 2),\n",
    "$$\n",
    "J_i:= J(W,b;\\mathbf{x}^{(i)},y^{(i)}) := - \\sum_{k=1}^{K} \\left\\{  1_{\\left\\{y^{(i)} = k\\right\\} }\n",
    "\\log P\\big(y^{(i)} = k \\;| \\;\\mathbf{a}^{(i)}; W^{(1)}\\big)\\right\\}. \\tag{1}\n",
    "$$\n",
    "\n",
    "Denote the data sample matrix $X := (\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)})^{\\top}$, its label vector as $\\mathbf{y} := (y^{(1)}, \\dots, y^{(N)})$, and then the final loss has an extra $L^2$-regularization term for the weight matrices (not for bias): \n",
    "$$\n",
    "L(W,b; X, \\mathbf{y}) := \\frac{1}{N}\\sum_{i=1}^{N} J_i  + \\frac{\\alpha}{2} \\Big(\\|W^{(0)}\\|^2 + \\|W^{(1)}\\|^2\\Big),\n",
    "\\tag{2}\n",
    "$$\n",
    "where $\\alpha>0$ is a hyper-parameter determining the strength of the regularization, the bigger the $\\alpha$ is, the smaller the magnitudes of the weights will be after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,W,b):\n",
    "    '''\n",
    "    Hypothesis function: simple FNN with 1 hidden layer\n",
    "    Layer 1: input\n",
    "    Layer 2: hidden layer, with a size implied by the arguments W[0], b\n",
    "    Layer 3: output layer, with a size implied by the arguments W[1]\n",
    "    '''\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    \n",
    "    # add one more layer if applicable\n",
    "    \n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    # the output is a probability for each sample\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X_in,weights):\n",
    "    '''\n",
    "    Un-used cell for demo\n",
    "    activation function for the last FC layer: softmax function \n",
    "    Output: K probabilities represent an estimate of P(y=k|X_in;weights) for k=1,...,K\n",
    "    the weights has shape (n, K)\n",
    "    n: the number of features X_in has\n",
    "    n = X_in.shape[1]\n",
    "    K: the number of classes\n",
    "    K = 10\n",
    "    '''\n",
    "    \n",
    "    s = np.exp(np.matmul(X_in,weights))\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    return s / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y_true):\n",
    "    '''\n",
    "    Loss function: cross entropy with an L^2 regularization\n",
    "    y_true: ground truth, of shape (N, )\n",
    "    y_pred: prediction made by the model, of shape (N, K) \n",
    "    N: number of samples in the batch\n",
    "    K: global variable, number of classes\n",
    "    '''\n",
    "    global K \n",
    "    K = 10\n",
    "    N = len(y_true)\n",
    "    # loss_sample stores the cross entropy for each sample in X\n",
    "    # convert y_true from labels to one-hot-vector encoding\n",
    "    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n",
    "    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n",
    "    # loss_sample is a dimension (N,) array\n",
    "    # for the final loss, we need take the average\n",
    "    return -np.mean(loss_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation (Chain rule)\n",
    "\n",
    "The derivative of the cross entropy $J$ in (1), for a single sample and its label $(\\mathbf{x}, y)$ , with respect to the weights and the bias is computed using the following procedure:\n",
    "> **Step 1**: Forward pass: computing the activations $\\mathbf{a} = (a_1,\\dots, a_{n_2})$ from the hidden layer (Layer 2), and $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots, \\sigma_K)$ from the output layer (Layer 3). \n",
    ">\n",
    "> **Step 2**: Derivatives for $W^{(1)}$: recall that $W^{(1)} = (\\boldsymbol{\\theta}_1 ,\\cdots,  \\boldsymbol{\\theta}_K)$ and denote \n",
    "$$\\mathbf{z}^{(2)} = \\big(z^{(2)}_1, \\dots, z^{(2)}_K\\big)  = (W^{(1)})^{\\top}\\mathbf{a} =\n",
    "\\big(\\boldsymbol{\\theta}^{\\top}_1 \\mathbf{a} ,\\cdots,  \\boldsymbol{\\theta}^{\\top}_K \\mathbf{a}\\big),$$ \n",
    "for the $k$-th output unit in the output layer (Layer 3), then\n",
    "$$\n",
    "\\delta^{(2)}_k\n",
    ":= \\frac{\\partial J}{\\partial z_k^{(2)}} = \\Big\\{  P\\big(y = k \\;| \\;\\mathbf{a}; W^{(1)}\\big)- 1_{\\{ y = k\\}} \\Big\\} = \\sigma_k - 1_{\\{ y = k\\}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}_k}= \\frac{\\partial J}{\\partial z_k^{(2)}}\\frac{\\partial z_k^{(2)}}{\\partial \\boldsymbol{\\theta}_k} = \\delta^{(2)}_k \\mathbf{a}.\n",
    "$$\n",
    ">\n",
    "> **Step 3**: Derivatives for $W^{(0)}$, $\\mathbf{b}$: recall that $W^{(0)} = (\\boldsymbol{w}_1 ,\\cdots,  \\boldsymbol{w}_{n_2})$, $\\mathbf{b} = (b_1,\\dots, b_{n_2})$, where $n_2$ is the number of units in the hidden layer (Layer 2), and denote \n",
    "$$\\mathbf{z}^{(1)} = (z_1^{(1)}, \\dots, z_{n_2}^{(1)})  = (W^{(0)})^{\\top}\\mathbf{x} + \\mathbf{b} =\n",
    "\\big(\\mathbf{w}^{\\top}_1 \\mathbf{x} +b_1 ,\\cdots,  \\mathbf{w}^{\\top}_{n_2} \\mathbf{x} + b_{n_2}\\big),$$ \n",
    "for each node $i$ in the hidden layer (Layer $2$), $i=1,\\dots, n_2$, then\n",
    "$$\\delta^{(1)}_i : = \\frac{\\partial J}{\\partial z^{(1)}_i}  =\n",
    "\\frac{\\partial J}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}=\n",
    "\\frac{\\partial J}{\\partial \\mathbf{z}^{(2)}}\n",
    "\\cdot\\left(\\frac{\\partial \\mathbf{z}^{(2)}}{\\partial a_i} \n",
    "\\frac{\\partial a_i}{\\partial z^{(1)}_i}\\right)\n",
    "\\\\\n",
    "=\\left( \\sum_{k=1}^{K} \\frac{\\partial J}{\\partial {z}^{(2)}_k}\n",
    "\\frac{\\partial {z}^{(2)}_k}{\\partial a_i}  \\right) f'(z^{(1)}_i) = \\left( \\sum_{k=1}^{K} w_{ki} \\delta^{(2)}_k \\right) 1_{\\{z^{(1)}_i\\; > 0\\}},\n",
    "$$\n",
    "where $1_{\\{z^{(1)}_i\\; > 0\\}}$ is ReLU activation $f$'s (weak) derivative, and the partial derivative of the $k$-th component (before activated by the softmax) in the output layer ${z}^{(2)}_k$ with respect to the $i$-th activation $a_i$ from the hidden layer is the weight $w^{(1)}_{ki}$. Thus\n",
    ">\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_{ji}}  = x_j \\delta_i^{(1)} ,\\;\n",
    "\\frac{\\partial J}{\\partial b_{i}} = \\delta_i^{(1)}, \\;\\text{ and }\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}_{i}}  = \\delta_i^{(1)}\\mathbf{x} ,\\;\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}} = \\boldsymbol{\\delta}^{(1)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(W,b,X,y,alpha=1e-4):\n",
    "    '''\n",
    "    Step 1: explicit forward pass h(X;W,b)\n",
    "    Step 2: backpropagation for dW and db\n",
    "    '''\n",
    "    K = 10\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    ### Step 1:\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    # layer 2 activation\n",
    "    a2 = relu(z1)\n",
    "    \n",
    "    # one more layer\n",
    "    \n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z2 = np.matmul(a2, W[1])\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    \n",
    "    ### Step 2:\n",
    "    \n",
    "    # layer 2->layer 3 weights' derivative\n",
    "    # delta2 is \\partial L/partial z2, of shape (N,K)\n",
    "    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n",
    "    delta2 = (sigma - y_one_hot_vec)\n",
    "    grad_W1 = np.matmul(a2.T, delta2)\n",
    "    \n",
    "    # layer 1->layer 2 weights' derivative\n",
    "    # delta1 is \\partial a2/partial z1\n",
    "    # layer 2 activation's (weak) derivative is 1*(z1>0)\n",
    "    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n",
    "    grad_W0 = np.matmul(X.T, delta1)\n",
    "    \n",
    "    # Possible student project: extra layer of derivative\n",
    "    \n",
    "    # no derivative for layer 1\n",
    "    \n",
    "    # the alpha part is the derivative for the regularization\n",
    "    # regularization = 0.5*alpha*(np.sum(W[1]**2) + np.sum(W[0]**2))\n",
    "    \n",
    "    \n",
    "    dW = [grad_W0/N + alpha*W[0], grad_W1/N + alpha*W[1]]\n",
    "    db = [np.mean(delta1, axis=0)]\n",
    "    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters and network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 5e-1\n",
    "alpha = 1e-6 # regularization\n",
    "gamma = 0.99 # RMSprop\n",
    "eps = 1e-3 # RMSprop\n",
    "num_iter = 1000 # number of iterations of gradient descent\n",
    "n_H = 256 # number of neurons in the hidden layer\n",
    "n = X_train.shape[1] # number of pixels in an image\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "np.random.seed(1127825)\n",
    "W = [1e-1*np.random.randn(n, n_H), 1e-1*np.random.randn(n_H, K)]\n",
    "b = [np.random.randn(n_H)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: training of the network\n",
    "\n",
    "In the training, we use a GD-variant of the RMSprop: for $\\mathbf{w}$ which stands for the parameter vector in our model\n",
    "> Choose $\\mathbf{w}_0$, $\\eta$, $\\gamma$, $\\epsilon$, and let $g_{-1} = 1$ <br><br>\n",
    ">    For $k=0,1,2, \\cdots, M$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;  $g_{k} = \\gamma g_{k-1} + (1 - \\gamma)\\, \\left|\\partial_{\\mathbf{w}} L (\\mathbf{w}_k)\\right|^2$<br><br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\mathbf{w}_{k+1} =  \\mathbf{w}_k -  \\frac{\\eta} {\\sqrt{g_{k}+ \\epsilon}} \\partial_{\\mathbf{w}} L(\\mathbf{w}_k)$  \n",
    "\n",
    "### Remark: \n",
    "The training takes a while since we use the gradient descent for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss after 1 iterations is 7.6743264\n",
      "Training accuracy after 1 iterations is 24.9262%\n",
      "gW0=1.0839 gW1=1.3825 gb0=0.9923\n",
      "etaW0=0.4800 etaW1=0.4251 etab0=0.5017\n",
      "|dW0|=3.06418 |dW1|=6.26530 |db0|=0.48001 \n",
      "\n",
      "Cross-entropy loss after 501 iterations is 0.13586252\n",
      "Training accuracy after 501 iterations is 96.1429%\n",
      "gW0=0.3495 gW1=0.1121 gb0=0.0153\n",
      "etaW0=0.8446 etaW1=1.4870 etab0=3.9189\n",
      "|dW0|=0.05204 |dW1|=0.02360 |db0|=0.00697 \n",
      "\n",
      "Final cross-entropy loss is 0.065978804\n",
      "Final training accuracy is 98.1000%\n",
      "CPU times: user 1h 2min 50s, sys: 19min 46s, total: 1h 22min 37s\n",
      "Wall time: 22min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gW0 = gW1 = gb0 = 1\n",
    "\n",
    "for i in range(num_iter):\n",
    "    dW, db = backprop(W,b,X_train,y_train,alpha)\n",
    "    \n",
    "    gW0 = gamma*gW0 + (1-gamma)*np.sum(dW[0]**2)\n",
    "    etaW0 = eta/np.sqrt(gW0 + eps)\n",
    "    W[0] -= etaW0 * dW[0]\n",
    "    \n",
    "    gW1 = gamma*gW1 + (1-gamma)*np.sum(dW[1]**2)\n",
    "    etaW1 = eta/np.sqrt(gW1 + eps)\n",
    "    W[1] -= etaW1 * dW[1]\n",
    "    \n",
    "    gb0 = gamma*gb0 + (1-gamma)*np.sum(db[0]**2)\n",
    "    etab0 = eta/np.sqrt(gb0 + eps)\n",
    "    b[0] -= etab0 * db[0]\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        # sanity check 1\n",
    "        y_pred = h(X_train,W,b)\n",
    "        print(\"Cross-entropy loss after\", i+1, \"iterations is {:.8}\".format(\n",
    "              loss(y_pred,y_train)))\n",
    "        print(\"Training accuracy after\", i+1, \"iterations is {:.4%}\".format( \n",
    "              np.mean(np.argmax(y_pred, axis=1)== y_train)))\n",
    "        \n",
    "        # sanity check 2\n",
    "        print(\"gW0={:.4f} gW1={:.4f} gb0={:.4f}\\netaW0={:.4f} etaW1={:.4f} etab0={:.4f}\"\n",
    "              .format(gW0, gW1, gb0, etaW0, etaW1, etab0))\n",
    "        \n",
    "        # sanity check 3\n",
    "        print(\"|dW0|={:.5f} |dW1|={:.5f} |db0|={:.5f}\"\n",
    "             .format(np.linalg.norm(dW[0]), np.linalg.norm(dW[1]), np.linalg.norm(db[0])), \"\\n\")\n",
    "        \n",
    "        # reset RMSprop\n",
    "        gW0 = gW1 = gb0 = 1\n",
    "\n",
    "y_pred_final = h(X_train,W,b)\n",
    "print(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\n",
    "print(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for testing data\n",
    "The prediction labels are generated by $(\\ast)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_test = np.argmax(h(X_test,W,b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating submission using pandas for grading\n",
    "submission = pd.DataFrame({'ImageId': range(1,len(X_test)+1) ,'Label': y_pred_test })\n",
    "submission.to_csv(\"simplemnist_result.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
